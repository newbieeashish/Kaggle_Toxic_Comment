{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os,re,codecs,csv\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras import optimizers\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/'\n",
    "comp = 'kaggle_toxic_comment/jigsaw-toxic-comment-classification-challenge/'\n",
    "\n",
    "EMBEDDING_FILE=f'D:/kaggle_fake_news/glove.6B/glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE=f'D:/kaggle_toxic_comment/jigsaw-toxic-comment-classification-challenge/train/train.csv'\n",
    "TEST_DATA_FILE=f'D:/kaggle_toxic_comment/jigsaw-toxic-comment-classification-challenge/test/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50     # how big is each word vector\n",
    "max_features = 20000   # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100    #max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/kaggle_toxic_comment/jigsaw-toxic-comment-classification-challenge/train/train.csv'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:/kaggle_toxic_comment/jigsaw-toxic-comment-classification-challenge/train/train.csv')\n",
    "test = pd.read_csv('D:/kaggle_toxic_comment/jigsaw-toxic-comment-classification-challenge/test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Explanation\\nWhy the edits made under my usern...\n",
       "1         D'aww! He matches this background colour I'm s...\n",
       "2         Hey man, I'm really not trying to edit war. It...\n",
       "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4         You, sir, are my hero. Any chance you remember...\n",
       "                                ...                        \n",
       "159566    \":::::And for the second time of asking, when ...\n",
       "159567    You should be ashamed of yourself \\n\\nThat is ...\n",
       "159568    Spitzer \\n\\nUmm, theres no actual article for ...\n",
       "159569    And it looks like it was actually you who put ...\n",
       "159570    \"\\nAnd ... I really don't think you understand...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Yo bitch Ja Rule is more succesful then you'll...\n",
       "1         == From RfC == \\n\\n The title is fine as it is...\n",
       "2         \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3         :If you have a look back at the source, the in...\n",
       "4                 I don't anonymously edit articles at all.\n",
       "                                ...                        \n",
       "153159    . \\n i totally agree, this stuff is nothing bu...\n",
       "153160    == Throw from out field to home plate. == \\n\\n...\n",
       "153161    \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162    \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163    \" \\n :::Stop already. Your bullshit is not wel...\n",
       "Name: comment_text, Length: 153164, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_sentences_train\n",
    "train_sentences = train['comment_text']\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "test_sentences = test['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_sentences))\n",
    "\n",
    "\n",
    "list_tokenizer_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "list_tokenizer_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "\n",
    "X_t  = pad_sequences(list_tokenizer_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenizer_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype = 'float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE,encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4488/4488 [==============================] - 3509s 782ms/step - loss: 0.0595 - accuracy: 0.9417 - val_loss: 0.0499 - val_accuracy: 0.9940\n",
      "Epoch 2/2\n",
      "4488/4488 [==============================] - 4341s 967ms/step - loss: 0.0444 - accuracy: 0.9774 - val_loss: 0.0457 - val_accuracy: 0.9940\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
